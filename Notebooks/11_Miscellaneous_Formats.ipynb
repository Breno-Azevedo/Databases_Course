{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Databases\n",
    "\n",
    "### Miscellaneous Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../SampleDBs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datapath, 'newfile.txt'), mode='w', encoding='utf-8') as f:\n",
    "    f.write('Weight\\t\\t72\\n')\n",
    "    f.write('Height\\t\\t183\\n')\n",
    "    f.write('Age\\t\\t44\\n')\n",
    "    f.write('Gender\\t\\tMasculine\\n')\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datapath, 'newfile.txt'), mode='r', encoding='utf-8') as f:\n",
    "    fulltext = f.read()\n",
    "    f.seek(0)\n",
    "    line = f.readline()\n",
    "    f.seek(0)\n",
    "    list_of_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\t\t72\n",
      "Height\t\t183\n",
      "Age\t\t44\n",
      "Gender\t\tMasculine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\t\t72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weight\\t\\t72\\n', 'Height\\t\\t183\\n', 'Age\\t\\t44\\n', 'Gender\\t\\tMasculine\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing file\n",
    "os.remove(os.path.join(datapath, 'newfile.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Numpy Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05  0.055\n",
      " 0.06  0.065 0.07  0.075 0.08  0.085 0.09  0.095 0.1   0.105 0.11  0.115\n",
      " 0.12  0.125 0.13  0.135 0.14  0.145 0.15  0.155 0.16  0.165 0.17  0.175\n",
      " 0.18  0.185 0.19  0.195 0.2   0.205 0.21  0.215 0.22  0.225 0.23  0.235\n",
      " 0.24  0.245 0.25  0.255 0.26  0.265 0.27  0.275 0.28  0.285 0.29  0.295\n",
      " 0.3   0.305 0.31  0.315 0.32  0.325 0.33  0.335 0.34  0.345 0.35  0.355\n",
      " 0.36  0.365 0.37  0.375 0.38  0.385 0.39  0.395 0.4   0.405 0.41  0.415\n",
      " 0.42  0.425 0.43  0.435 0.44  0.445 0.45  0.455 0.46  0.465 0.47  0.475\n",
      " 0.48  0.485 0.49  0.495 0.5   0.505 0.51  0.515 0.52  0.525 0.53  0.535\n",
      " 0.54  0.545 0.55  0.555 0.56  0.565 0.57  0.575 0.58  0.585 0.59  0.595\n",
      " 0.6   0.605 0.61  0.615 0.62  0.625 0.63  0.635 0.64  0.645 0.65  0.655\n",
      " 0.66  0.665 0.67  0.675 0.68  0.685 0.69  0.695 0.7   0.705 0.71  0.715\n",
      " 0.72  0.725 0.73  0.735 0.74  0.745 0.75  0.755 0.76  0.765 0.77  0.775\n",
      " 0.78  0.785 0.79  0.795 0.8   0.805 0.81  0.815 0.82  0.825 0.83  0.835\n",
      " 0.84  0.845 0.85  0.855 0.86  0.865 0.87  0.875 0.88  0.885 0.89  0.895\n",
      " 0.9   0.905 0.91  0.915 0.92  0.925 0.93  0.935 0.94  0.945 0.95  0.955\n",
      " 0.96  0.965 0.97  0.975 0.98  0.985 0.99  0.995 1.   ]\n"
     ]
    }
   ],
   "source": [
    "data = np.linspace(0,1,201)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(datapath, 'data.dat'), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05  0.055\n",
      " 0.06  0.065 0.07  0.075 0.08  0.085 0.09  0.095 0.1   0.105 0.11  0.115\n",
      " 0.12  0.125 0.13  0.135 0.14  0.145 0.15  0.155 0.16  0.165 0.17  0.175\n",
      " 0.18  0.185 0.19  0.195 0.2   0.205 0.21  0.215 0.22  0.225 0.23  0.235\n",
      " 0.24  0.245 0.25  0.255 0.26  0.265 0.27  0.275 0.28  0.285 0.29  0.295\n",
      " 0.3   0.305 0.31  0.315 0.32  0.325 0.33  0.335 0.34  0.345 0.35  0.355\n",
      " 0.36  0.365 0.37  0.375 0.38  0.385 0.39  0.395 0.4   0.405 0.41  0.415\n",
      " 0.42  0.425 0.43  0.435 0.44  0.445 0.45  0.455 0.46  0.465 0.47  0.475\n",
      " 0.48  0.485 0.49  0.495 0.5   0.505 0.51  0.515 0.52  0.525 0.53  0.535\n",
      " 0.54  0.545 0.55  0.555 0.56  0.565 0.57  0.575 0.58  0.585 0.59  0.595\n",
      " 0.6   0.605 0.61  0.615 0.62  0.625 0.63  0.635 0.64  0.645 0.65  0.655\n",
      " 0.66  0.665 0.67  0.675 0.68  0.685 0.69  0.695 0.7   0.705 0.71  0.715\n",
      " 0.72  0.725 0.73  0.735 0.74  0.745 0.75  0.755 0.76  0.765 0.77  0.775\n",
      " 0.78  0.785 0.79  0.795 0.8   0.805 0.81  0.815 0.82  0.825 0.83  0.835\n",
      " 0.84  0.845 0.85  0.855 0.86  0.865 0.87  0.875 0.88  0.885 0.89  0.895\n",
      " 0.9   0.905 0.91  0.915 0.92  0.925 0.93  0.935 0.94  0.945 0.95  0.955\n",
      " 0.96  0.965 0.97  0.975 0.98  0.985 0.99  0.995 1.   ]\n"
     ]
    }
   ],
   "source": [
    "new_data = np.loadtxt(os.path.join(datapath, 'data.dat'))\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing file\n",
    "os.remove(os.path.join(datapath, 'data.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.19087582]]\n",
      "[[0.05263158 0.67513309]]\n",
      "[[0.10526316 0.9181597 ]]\n",
      "[[0.15789474 0.50386329]]\n",
      "[[0.21052632 0.00413998]]\n",
      "[[0.26315789 0.73405658]]\n",
      "[[0.31578947 0.53191564]]\n",
      "[[0.36842105 0.14263867]]\n",
      "[[0.42105263 0.4126989 ]]\n",
      "[[0.47368421 0.6598822 ]]\n",
      "[[0.52631579 0.02803298]]\n",
      "[[0.57894737 0.06654837]]\n",
      "[[0.63157895 0.71172667]]\n",
      "[[0.68421053 0.61800692]]\n",
      "[[0.73684211 0.30624494]]\n",
      "[[0.78947368 0.64367149]]\n",
      "[[0.84210526 0.33472929]]\n",
      "[[0.89473684 0.2106857 ]]\n",
      "[[0.94736842 0.83396408]]\n",
      "[[1.         0.30411785]]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 1, 20)\n",
    "y = np.random.random(20)\n",
    "header = \"X-Column, Y-Column\\n\"\n",
    "header += \"This is a second line\"\n",
    "\n",
    "with open(os.path.join(datapath,'data2.dat'), 'wb') as f:\n",
    "#with open(os.path.join(datapath,'data2.dat'), 'w') as f:   #using text formatting and no binary files\n",
    "    np.savetxt(f, [], header=header)\n",
    "    for i in range(20):\n",
    "        data = np.column_stack((x[i], y[i]))\n",
    "        np.savetxt(f, data), print(data)\n",
    "        #f.write('{:4.1f} {:.4f}\\n'.format(x[i], y[i]))    #using text formatting and no binary files\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# X-Column, Y-Column\n",
      "# This is a second line\n",
      "0.000000000000000000e+00 1.908758206248156730e-01\n",
      "5.263157894736841813e-02 6.751330943973540277e-01\n",
      "1.052631578947368363e-01 9.181596973028762765e-01\n",
      "1.578947368421052544e-01 5.038632936753323355e-01\n",
      "2.105263157894736725e-01 4.139984396025475633e-03\n",
      "2.631578947368420907e-01 7.340565775051863806e-01\n",
      "3.157894736842105088e-01 5.319156420087179926e-01\n",
      "3.684210526315789269e-01 1.426386737837259666e-01\n",
      "4.210526315789473450e-01 4.126989046288584984e-01\n",
      "4.736842105263157632e-01 6.598822014163375771e-01\n",
      "5.263157894736841813e-01 2.803297691725059604e-02\n",
      "5.789473684210526550e-01 6.654836761361238029e-02\n",
      "6.315789473684210176e-01 7.117266739991724434e-01\n",
      "6.842105263157893802e-01 6.180069205384882691e-01\n",
      "7.368421052631578538e-01 3.062449414130001824e-01\n",
      "7.894736842105263275e-01 6.436714908043307259e-01\n",
      "8.421052631578946901e-01 3.347292875734768636e-01\n",
      "8.947368421052630527e-01 2.106856992477605939e-01\n",
      "9.473684210526315264e-01 8.339640796131211342e-01\n",
      "1.000000000000000000e+00 3.041178484861216669e-01\n"
     ]
    }
   ],
   "source": [
    "! cat ../SampleDBs/data2.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing file\n",
    "os.remove(os.path.join(datapath, 'data2.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = [1, 1.2, 'a', 'b']\n",
    "\n",
    "with open(os.path.join(datapath, 'data.pkl'), 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.2, 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(datapath, 'data.pkl'), 'rb') as f:\n",
    "    new_data = pickle.load(f)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [1, 1.2, 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(datapath, 'data.pkl.gz'), 'wb') as f:\n",
    "    pickle.dump(data2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(datapath, 'data.pkl.gz'), 'rb') as f:\n",
    "    new_data2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1.2, 'a', 'b']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing files\n",
    "os.remove(os.path.join(datapath, 'data.pkl.gz'))\n",
    "os.remove(os.path.join(datapath, 'data.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = \"fakedata.csv\"\n",
    "datafile = os.path.join(datapath, csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Fatima',\n",
       "  'email': 'Quisque.varius@Integervitae.org',\n",
       "  'birthdate': '11-25-01'},\n",
       " {'name': 'Katelyn',\n",
       "  'email': 'mi.pede.nonummy@Sedid.ca',\n",
       "  'birthdate': '11-27-03'},\n",
       " {'name': 'Gillian',\n",
       "  'email': 'odio.semper@sodalesMaurisblandit.org',\n",
       "  'birthdate': '02-19-14'},\n",
       " {'name': 'Preston',\n",
       "  'email': 'faucibus.orci@lacusQuisque.edu',\n",
       "  'birthdate': '08-07-09'},\n",
       " {'name': 'Priscilla',\n",
       "  'email': 'semper.auctor@cursusvestibulum.co.uk',\n",
       "  'birthdate': '08-12-01'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open(datafile, \"r\") as f:\n",
    "    keys = f.readline().split('|')\n",
    "    keys = [k.strip() for k in keys]\n",
    "    for i in range(5):\n",
    "        values = f.readline().split('|')\n",
    "        values = [v.strip() for v in values]\n",
    "        d = dict(zip(keys,values))\n",
    "        data.append(d)\n",
    "data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using [CSV](https://docs.python.org/3/library/csv.html) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'email', 'birthdate']\n",
      "['Fatima', 'Quisque.varius@Integervitae.org', '11-25-01']\n",
      "['Katelyn', 'mi.pede.nonummy@Sedid.ca', '11-27-03']\n",
      "['Gillian', 'odio.semper@sodalesMaurisblandit.org', '02-19-14']\n",
      "['Preston', 'faucibus.orci@lacusQuisque.edu', '08-07-09']\n",
      "['Priscilla', 'semper.auctor@cursusvestibulum.co.uk', '08-12-01']\n",
      "['Zena', 'ante@magnaPraesent.com', '11-24-11']\n",
      "['Oren', 'rutrum.eu.ultrices@nec.org', '01-12-07']\n",
      "['Jamalia', 'Phasellus.vitae.mauris@vel.org', '05-17-17']\n",
      "['Libby', 'velit.eu@Maecenasmi.edu', '07-29-17']\n",
      "['Finn', 'natoque.penatibus@lectusa.net', '06-23-10']\n",
      "['Graiden', 'neque.Nullam@hendreritidante.com', '09-18-10']\n",
      "['Kitra', 'quis.tristique@estmollis.com', '11-18-02']\n",
      "['Baxter', 'vitae.mauris@sed.org', '04-12-17']\n",
      "['Margaret', 'ullamcorper@nec.ca', '08-16-04']\n",
      "['Sopoline', 'vitae.diam@velitQuisquevarius.co.uk', '09-03-06']\n",
      "['Flynn', 'eget.varius@magnased.org', '08-28-14']\n",
      "['Beck', 'arcu.et.pede@idblandit.co.uk', '01-14-06']\n",
      "['Sheila', 'natoque.penatibus.et@mattis.com', '09-01-20']\n",
      "['Francis', 'ligula.Aliquam@nullaatsem.ca', '08-25-14']\n",
      "['Harrison', 'nec.urna@loremauctorquis.com', '02-10-16']\n",
      "['Imogene', 'malesuada.id@nibh.com', '04-19-03']\n",
      "['Moana', 'sit@Morbinequetellus.net', '07-05-03']\n",
      "['Kamal', 'pede.Nunc.sed@Proindolor.ca', '01-14-01']\n",
      "['Fiona', 'nulla.at@Sedmalesuadaaugue.net', '02-07-01']\n",
      "['Yuri', 'pharetra@cursuspurusNullam.edu', '12-03-19']\n",
      "['Nichole', 'aliquet@euenim.org', '04-06-03']\n",
      "['Irma', 'elit@pedeCras.net', '04-29-18']\n",
      "['Leilani', 'et.ultrices@semegestasblandit.edu', '10-25-14']\n",
      "['Rashad', 'imperdiet.nec@convallisligulaDonec.co.uk', '02-09-11']\n",
      "['Jerry', 'augue@imperdiet.edu', '08-28-02']\n",
      "['Thomas', 'mauris@utodio.com', '09-30-10']\n",
      "['Yvette', 'ligula@Aliquam.edu', '03-03-04']\n",
      "['Hashim', 'Integer.sem@pede.co.uk', '07-29-07']\n",
      "['Theodore', 'ut.pharetra@egetipsumDonec.org', '08-11-05']\n",
      "['Basil', 'facilisis.vitae@Curae.ca', '08-31-15']\n",
      "['Blythe', 'lacinia@vehiculaPellentesquetincidunt.co.uk', '10-05-02']\n",
      "['Suki', 'Suspendisse.commodo.tincidunt@velit.co.uk', '04-17-07']\n",
      "['Mia', 'vulputate.velit@etmagnisdis.org', '08-03-07']\n",
      "['Chastity', 'venenatis.lacus.Etiam@quisurna.edu', '12-07-05']\n",
      "['Natalie', 'at.libero@feugiatplacerat.org', '06-25-14']\n",
      "['Scarlet', 'ac.fermentum@ullamcorpereu.edu', '09-13-00']\n",
      "['Gage', 'dolor.Fusce@ametlorem.edu', '03-25-20']\n",
      "['Miranda', 'auctor.nunc@magnisdis.ca', '09-20-02']\n",
      "['Nicole', 'elit@egestasSed.co.uk', '09-08-15']\n",
      "['Tatyana', 'nec.urna@sagittis.co.uk', '09-24-12']\n",
      "['Marcia', 'consectetuer@fringilla.co.uk', '05-12-19']\n",
      "['Germaine', 'eget.ipsum@dui.ca', '09-08-04']\n",
      "['Keith', 'Sed.et@nisimagna.net', '12-20-05']\n",
      "['Zephr', 'Praesent.interdum.ligula@eu.edu', '06-17-17']\n",
      "['Preston', 'accumsan.convallis.ante@sociisnatoquepenatibus.ca', '08-30-08']\n",
      "['Stone', 'elit.sed@leoinlobortis.ca', '09-30-02']\n",
      "['Phelan', 'Nam.ligula.elit@Sedpharetra.ca', '03-29-02']\n",
      "['Colton', 'risus.Donec@blanditcongueIn.co.uk', '04-28-11']\n",
      "['Stuart', 'senectus.et.netus@sit.com', '03-24-09']\n",
      "['Brendan', 'enim.mi.tempor@nectempusscelerisque.edu', '09-16-20']\n",
      "['Eden', 'velit.egestas.lacinia@non.com', '07-13-04']\n",
      "['Geoffrey', 'Donec.egestas@ullamcorperDuis.co.uk', '11-02-11']\n",
      "['Xantha', 'libero.at.auctor@mauris.ca', '09-17-00']\n",
      "['Dylan', 'primis.in.faucibus@tortorIntegeraliquam.net', '08-25-20']\n",
      "['Claire', 'pede@sagittissemperNam.ca', '05-23-13']\n",
      "['Hayden', 'sed@Nam.org', '02-24-09']\n",
      "['Russell', 'laoreet@infaucibus.net', '03-06-04']\n",
      "['Ingrid', 'tincidunt.nibh@disparturient.co.uk', '12-23-05']\n",
      "['Michael', 'orci.consectetuer@porttitorscelerisqueneque.com', '06-30-18']\n",
      "['Elton', 'sem.molestie.sodales@vitaesemperegestas.edu', '03-07-01']\n",
      "['Zena', 'sed@malesuadamalesuadaInteger.net', '12-23-14']\n",
      "['Emi', 'dictum.eu@lorem.org', '06-15-02']\n",
      "['Walter', 'non@loremacrisus.edu', '08-27-08']\n",
      "['Jason', 'scelerisque.dui@maurissitamet.net', '08-30-19']\n",
      "['Shannon', 'elit@sempercursus.net', '10-31-13']\n",
      "['Seth', 'Aliquam.nec@eu.ca', '10-30-09']\n",
      "['Magee', 'dui@odiosagittis.com', '11-01-02']\n",
      "['Kasper', 'Praesent@Proinvel.edu', '08-02-18']\n",
      "['Chester', 'et@orciPhasellus.org', '11-04-07']\n",
      "['Cadman', 'a.sollicitudin@enimconsequat.com', '12-03-15']\n",
      "['Erich', 'ligula.eu@sociosquadlitora.org', '10-07-06']\n",
      "['Allegra', 'lorem@Suspendisse.edu', '02-05-08']\n",
      "['Quemby', 'Curae.Donec@imperdietnonvestibulum.co.uk', '09-28-06']\n",
      "['Beverly', 'ullamcorper@temporeratneque.net', '11-17-08']\n",
      "['Kylee', 'luctus.felis.purus@variusultrices.net', '01-16-04']\n",
      "['Aidan', 'Curae@sempereratin.net', '10-15-08']\n",
      "['India', 'luctus@et.com', '08-03-02']\n",
      "['Asher', 'dictum.Proin@in.edu', '07-02-07']\n",
      "['Illana', 'interdum.libero@sitamet.com', '08-05-21']\n",
      "['Maxine', 'enim.commodo.hendrerit@consequat.com', '08-04-02']\n",
      "['Rashad', 'convallis.in@dapibusquam.org', '07-19-20']\n",
      "['Pearl', 'facilisis.Suspendisse@arcuMorbi.co.uk', '05-13-10']\n",
      "['Yael', 'facilisis.eget.ipsum@placerat.edu', '09-19-17']\n",
      "['Michael', 'facilisi.Sed@consequat.co.uk', '07-08-21']\n",
      "['Lawrence', 'luctus.Curabitur@lectusantedictum.ca', '06-02-09']\n",
      "['Raja', 'et.rutrum@orciinconsequat.edu', '02-13-11']\n",
      "['Irene', 'fermentum@maurisut.com', '09-03-02']\n",
      "['Perry', 'velit@Pellentesqueut.co.uk', '09-30-08']\n",
      "['Kellie', 'elit.Curabitur@erat.net', '07-08-02']\n",
      "['Quemby', 'sed@sedest.ca', '07-09-15']\n",
      "['Adrienne', 'Sed@faucibus.edu', '03-06-01']\n",
      "['Quintessa', 'Aliquam.adipiscing@urna.org', '08-21-07']\n",
      "['Baxter', 'blandit.at.nisi@Donec.edu', '01-15-01']\n",
      "['Rylee', 'at@cursus.net', '07-12-16']\n",
      "['Keegan', 'eu.lacus.Quisque@Inscelerisquescelerisque.net', '12-23-05']\n"
     ]
    }
   ],
   "source": [
    "with open(datafile, \"r\") as f:\n",
    "    data2 = csv.reader(f, delimiter='|')\n",
    "    #data2 = csv.DictReader(f, delimiter='|')\n",
    "    for row in data2:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fatima</td>\n",
       "      <td>Quisque.varius@Integervitae.org</td>\n",
       "      <td>11-25-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Katelyn</td>\n",
       "      <td>mi.pede.nonummy@Sedid.ca</td>\n",
       "      <td>11-27-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gillian</td>\n",
       "      <td>odio.semper@sodalesMaurisblandit.org</td>\n",
       "      <td>02-19-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preston</td>\n",
       "      <td>faucibus.orci@lacusQuisque.edu</td>\n",
       "      <td>08-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Priscilla</td>\n",
       "      <td>semper.auctor@cursusvestibulum.co.uk</td>\n",
       "      <td>08-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                 email birthdate\n",
       "0     Fatima       Quisque.varius@Integervitae.org  11-25-01\n",
       "1    Katelyn              mi.pede.nonummy@Sedid.ca  11-27-03\n",
       "2    Gillian  odio.semper@sodalesMaurisblandit.org  02-19-14\n",
       "3    Preston        faucibus.orci@lacusQuisque.edu  08-07-09\n",
       "4  Priscilla  semper.auctor@cursusvestibulum.co.uk  08-12-01"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = pd.read_csv(datafile, sep=\"|\")\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   name       100 non-null    object\n",
      " 1   email      100 non-null    object\n",
      " 2   birthdate  100 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating [Zip Files](https://docs.python.org/3/library/zipfile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressedfile = \"2013_ERCOT_Hourly_Load_Data.zip\"\n",
    "datafile = os.path.join(datapath, compressedfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_excel = pd.read_excel(datafile, com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013_ERCOT_Hourly_Load_Data.xls']\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "seek",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f575d7bbfddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0munzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munzipped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpd_excel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pyxlsb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpd_excel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/excel/_pyxlsb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# This will call load_workbook on the filepath or buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# And set the result to the book-attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;31m# N.B. xlrd.Book has a read attribute too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: seek"
     ]
    }
   ],
   "source": [
    "with ZipFile(datafile, 'r') as myzip:\n",
    "    print(myzip.namelist())\n",
    "    unzipped = myzip.namelist()[0]\n",
    "    with myzip.open(unzipped, 'r') as myfile:\n",
    "        pd_excel = pd.read_excel(myfile, engine='pyxlsb')\n",
    "        \n",
    "pd_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Acessing](https://pypi.python.org/pypi/xlrd) and [Writing](https://pypi.python.org/pypi/xlwt) [Excel Files](https://github.com/python-excel/tutorial/blob/master/python-excel.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip install -U xlrd xlwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(datafile, 'r') as myzip:\n",
    "    unzipped = myzip.namelist()[0]\n",
    "    myzip.extractall(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlrd.open_workbook(os.path.join(datapath, unzipped))\n",
    "sheet = workbook.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "sheet_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in the sheet:\"),\n",
    "print(sheet.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of data in cell (row 3, col 2):\"), \n",
    "print(sheet.cell_type(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value in cell (row 3, col 2):\", )\n",
    "print(sheet.cell_value(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Get a slice of values in column 3, from rows 1-3:\",)\n",
    "print(sheet.col_values(3, start_rowx=1, end_rowx=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coast = sheet.col_values(1, start_rowx=1)\n",
    "data3 = {}    \n",
    "data3['maxvalue'] = max(coast)\n",
    "data3['minvalue'] = min(coast)\n",
    "data3['avgcoast'] = np.mean(coast)\n",
    "\n",
    "rowmax = coast.index(max(coast))+1\n",
    "rowmin = coast.index(min(coast))+1\n",
    "\n",
    "data3['maxtime'] = xlrd.xldate_as_tuple(sheet.cell_value(rowmax,0), 0)\n",
    "data3['mintime'] = xlrd.xldate_as_tuple(sheet.cell_value(rowmin,0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_excel = pd.read_excel(os.path.join(datapath, unzipped))\n",
    "pd_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing file\n",
    "os.remove(os.path.join(datapath, unzipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Writing a Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style0 = xlwt.easyxf('font: name Times New Roman, color-index red, bold on', num_format_str='#,##0.00')\n",
    "style1 = xlwt.easyxf(num_format_str='D-MMM-YY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = xlwt.Workbook()\n",
    "workbook_name = 'my_workbook.xls'\n",
    "\n",
    "worksheet1 = workbook.add_sheet('my_first_sheet')\n",
    "worksheet1.write(0, 0, 'Column 1 title', style0)\n",
    "worksheet1.write(0, 1, 'Column 2 title', style0)\n",
    "\n",
    "worksheet2 = workbook.add_sheet('my_second_sheet')\n",
    "worksheet2.write(0, 0, 'Itens')\n",
    "worksheet2.write(0, 1, 'Price')\n",
    "worksheet2.write(1, 0, 'Rice')\n",
    "worksheet2.write(1, 1, 100)\n",
    "worksheet2.write(2, 0, 'Beans')\n",
    "worksheet2.write(2, 1, 200)\n",
    "worksheet2.write(3, 0, 'Pasta')\n",
    "worksheet2.write(3, 1, 500)\n",
    "worksheet2.write(5, 0, 'Total')\n",
    "\n",
    "worksheet2.write(5, 1, xlwt.Formula(\"B2+B3+B4\"))\n",
    "\n",
    "\n",
    "workbook.save(os.path.join(datapath, workbook_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing file\n",
    "os.remove(os.path.join(datapath, workbook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading PDF Files  \n",
    "\n",
    "More resources [here](https://textract.readthedocs.io/en/stable/) and [here](https://towardsdatascience.com/python-for-pdf-ef0fac2808b0?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip install -U textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, f)) and f.endswith('.pdf')]\n",
    "onlyfiles.sort()\n",
    "\n",
    "print('Files in the folder:')\n",
    "for i, w in enumerate(onlyfiles):\n",
    "    print(i, '--' ,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text0 = textract.process(os.path.join(datapath, onlyfiles[0])).decode('utf-8')\n",
    "print(text0[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text in Image Files (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install -y tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, f)) and f.endswith('.png')]\n",
    "onlyfiles.sort()\n",
    "\n",
    "print('Files in the folder:')\n",
    "for i, w in enumerate(onlyfiles):\n",
    "    print(i, '--' ,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = textract.process(os.path.join(datapath, onlyfiles[3])).decode('utf-8')\n",
    "\n",
    "print(text1[0:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/urllib.html  \n",
    "http://www.crummy.com/software/BeautifulSoup/  \n",
    "http://docs.python-requests.org/en/latest/  \n",
    "http://lxml.de/  \n",
    "https://docs.python.org/3/library/getpass.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml.html\n",
    "\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pt.wikipedia.org/wiki/Guimar%C3%A3es_Rosa\"\n",
    "html_txt = urllib.request.urlopen(url).read()\n",
    "soup = bs(html_txt, \"html.parser\")\n",
    " \n",
    "print(soup.text[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[line.get('href') for line in soup.find_all('a')][0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach: [lxml](http://lxml.de/tutorial.html) instead of beautiful soup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pt.wikipedia.org/wiki/Guimar%C3%A3es_Rosa\"\n",
    "html_txt = urllib.request.urlopen(url).read()\n",
    "dom =  lxml.html.fromstring(html_txt)\n",
    "    \n",
    "[line for line in dom.xpath('//a/@href')][0:10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using [Requests](http://docs.python-requests.org/en/master/user/quickstart/), for more complex tasks  \n",
    "\n",
    "\n",
    "c.f. [HTML Responses](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getpass.getpass()\n",
    "r = requests.get('https://api.github.com/user', auth=('rsouza', p))\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing [JSON](http://json.org/)\n",
    "\n",
    "https://docs.python.org/3/library/json.html  \n",
    "https://docs.python.org/3/library/urllib.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://musicbrainz.org/ws/2/\"\n",
    "artist_url =  urllib.parse.urljoin(base_url, 'artist/')\n",
    "query_type = {\"simple\": {},\n",
    "              \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "              \"aliases\": {\"inc\": \"aliases\"},\n",
    "              \"releases\": {\"inc\": \"releases\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print(\"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_name(url, params, name):\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data, indent=4):\n",
    "    if type(data) == dict:\n",
    "        print(json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_by_name(artist_url, query_type[\"simple\"], \"Nirvana\")\n",
    "pretty_print(results['artists'])  ##format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nARTIST:\")\n",
    "pretty_print(results[\"artists\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_id = results[\"artists\"][0][\"id\"]\n",
    "artist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_data = query_site(artist_url, query_type[\"releases\"], artist_id)\n",
    "releases = artist_data[\"releases\"]\n",
    "print(\"\\nONE RELEASE:\")\n",
    "pretty_print(releases[0], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_titles = [r[\"title\"] for r in releases]\n",
    "print(\"\\nALL TITLES:\")\n",
    "for t in release_titles:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')\n",
    "df_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datapath, 'simple.yaml'), 'r') as stream:\n",
    "    try:\n",
    "        yamlfile = yaml.load(stream, Loader=yaml.Loader)\n",
    "        print(yamlfile)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(yamlfile[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/urllib.html\n",
    "\n",
    "https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "\n",
    "http://lxml.de/\n",
    "\n",
    "http://www.w3schools.com/xml/xml_examples.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "#from xml.etree import ElementTree as ET\n",
    "from lxml import etree as ET #Supports xpath syntax\n",
    "from collections import defaultdict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.w3schools.com/xml/cd_catalog.xml'\n",
    "xml_page = urllib.request.urlopen(url).read()\n",
    "e = ET.XML(xml_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etree_to_dict(t):\n",
    "    d = {t.tag: {} if t.attrib else None}\n",
    "    children = list(t)\n",
    "    if children:\n",
    "        dd = defaultdict(list)\n",
    "        for dc in map(etree_to_dict, children):\n",
    "            for k, v in dc.items():\n",
    "                dd[k].append(v)\n",
    "        d = {t.tag: {k:v[0] if len(v) == 1 else v for k, v in dd.items()}}\n",
    "    if t.attrib:\n",
    "        d[t.tag].update(('@' + k, v) for k, v in t.attrib.items())\n",
    "    if t.text:\n",
    "        text = t.text.strip()\n",
    "        if children or t.attrib:\n",
    "            if text:\n",
    "              d[t.tag]['#text'] = text\n",
    "        else:\n",
    "            d[t.tag] = text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(etree_to_dict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_page = urllib.request.urlopen(url)\n",
    "doc = minidom.parse(raw_page)\n",
    "\n",
    "def findTextnodes(nodeList):\n",
    "    for subnode in nodeList:\n",
    "        if subnode.nodeType == subnode.ELEMENT_NODE:\n",
    "            print(\"Element node: \" + subnode.tagName)\n",
    "            findTextnodes(subnode.childNodes)\n",
    "        elif subnode.nodeType == subnode.TEXT_NODE:\n",
    "            print(\"text node:\" + subnode.data)\n",
    "            \n",
    "findTextnodes(doc.childNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.fromstring(xml_page)\n",
    "cdtags = root.xpath('//CD/TITLE')\n",
    "for cd in cdtags:\n",
    "    print(cd.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cd in root.findall('CD'):\n",
    "    title = cd.find('TITLE').text\n",
    "    artist = cd.find('ARTIST').text\n",
    "    country = cd.find('COUNTRY').text\n",
    "    print('{}\\t{}\\t{}'.format(artist, country, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = []\n",
    "for cd in root.findall('./CD'):\n",
    "        data = {}\n",
    "        data[\"TITLE\"] = cd.find('./TITLE').text\n",
    "        data[\"ARTIST\"] = cd.find('./ARTIST').text\n",
    "        data[\"COUNTRY\"] = cd.find('./COUNTRY').text\n",
    "        data[\"PRICE\"] = cd.find('./PRICE').text\n",
    "        data[\"YEAR\"] = cd.find('./YEAR').text\n",
    "        cds.append(data)\n",
    "cds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Feeds RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.feedparser.org/  \n",
    "http://docs.python.org/library/re.html  \n",
    "http://www.pythonware.com/library/pil/handbook/index.htm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = feedparser.parse('https://oglobo.globo.com/rss.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['feed']['title'])\n",
    "print(d['feed']['link'])\n",
    "#print(d.feed.subtitle)\n",
    "print(len(d['entries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['entries'][0]['title']) \n",
    "print(d.entries[0]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in d.entries:\n",
    "    print(post.title + \": \" + post.link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Big Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://eforexcel.com/wp/wp-content/uploads/2017/07/1500000%20Sales%20Records.zip'\n",
    "filename = \"bigcsv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(link, os.path.join(datapath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pandas with chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10 ** 5\n",
    "num_chunks_to_read = 3\n",
    "\n",
    "with ZipFile(os.path.join(datapath, filename), 'r') as myzip:\n",
    "    print(myzip.namelist())\n",
    "    unzipped = myzip.namelist()[0]\n",
    "    with myzip.open(unzipped) as myfile:\n",
    "        chunks = pd.read_csv((myfile), chunksize=chunksize) #, iterator=True)\n",
    "        for i in range(num_chunks_to_read):\n",
    "            print('Retrieving a new chunk')\n",
    "            df = chunks.get_chunk()\n",
    "            print(df.head(3))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping the file and reading line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(os.path.join(datapath, filename), 'r') as myzip:\n",
    "    myzip.extractall(datapath)\n",
    "    myzip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstuff(filename):\n",
    "    with open(filename, \"r\") as csvfile:\n",
    "        datareader = csv.reader(csvfile)\n",
    "        count = 0\n",
    "        for row in datareader:\n",
    "            yield(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for row in getstuff(os.path.join(datapath, unzipped)):\n",
    "    print(row)\n",
    "    count +=1\n",
    "    if count > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing files\n",
    "os.remove(os.path.join(datapath, unzipped))\n",
    "os.remove(os.path.join(datapath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using [HDF5](https://portal.hdfgroup.org/display/HDF5/HDF5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip install -q -U h5py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "arr = np.random.randn(1000)\n",
    "\n",
    "filename = 'random.hdf5'\n",
    "#with h5py.File(os.path.join(datapath, filename), 'a') as f:   #if we'd like to append data\n",
    "with h5py.File(os.path.join(datapath, filename), 'w') as f:\n",
    "    dset = f.create_dataset(\"default\", data=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(datapath, filename), 'r') as f:\n",
    "    for key in f.keys():\n",
    "        print(\"datasets in ths hdf system:\", key)\n",
    "    \n",
    "    data = f['default']\n",
    "    print(type(data))\n",
    "    print(min(data))\n",
    "    print(max(data))\n",
    "    print(data[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.random.randn(10000)\n",
    "arr2 = np.random.randn(10000)\n",
    "\n",
    "with h5py.File(os.path.join(datapath, filename), 'w') as f:\n",
    "    f.create_dataset('array_1', data=arr1)\n",
    "    f.create_dataset('array_2', data=arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(datapath, filename), 'r') as f:\n",
    "    d1 = f['array_1']\n",
    "    d2 = f['array_2']\n",
    "    print(type(d1))\n",
    "    print(type(d2))\n",
    "    #data = d2[d1>0]  #error\n",
    "    data = d2[d1[:]>0]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erasing files\n",
    "\n",
    "os.remove(os.path.join(datapath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://vaex.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.dask.org/en/latest/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.markdownguide.org/cheat-sheet/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
